# ğŸ•¸ï¸ Web Scraper Boilerplate

This is a reusable Python web scraping boilerplate designed for client projects. It includes structured folders, logging, file output, and testing support using `pytest`.

---

## ğŸš€ Features

- BeautifulSoup + Requests for scraping
- Modular structure for scalability
- Built-in logging
- Pytest support with config
- CSV output support
- Easy to copy & customize for client work

---

## ğŸ—‚ï¸ Folder Structure

```
web_scraper_boilerplate/
â”œâ”€â”€ data/                  # CSV or JSON output
â”œâ”€â”€ logs/                  # Log files
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py         # fetch_data() & parse_data()
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ file_writer.py # save_to_csv() or JSON
â”‚       â””â”€â”€ logger.py      # logging setup
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_scraper.py    # unit tests
â”œâ”€â”€ pytest.ini             # test config
â””â”€â”€ README.md              # project info
```

---

## ğŸ§ª How to Run Tests

Just run:

```
pytest
```

> Logging is enabled and console-friendly thanks to `pytest.ini`.

---

## ğŸ“ Logging

- All logs are written to `logs/scraper.log`
- You can change log level or format in `src/utils/logger.py`

---

## ğŸ§° How to Use

1. Replace the sample logic in `fetch_data()` and `parse_data()` in `src/scraper.py` with your real target.
2. Use `save_to_csv()` from `file_writer.py` to store results.
3. Add more test cases in `tests/test_scraper.py` as needed.

---

## ğŸ“¦ Dependencies

Install required packages:

```bash
pip install -r requirements.txt
```

---

## ğŸ” Reuse

To reuse this for a new project:

1. Duplicate this folder
2. Rename it to the client/project name
3. Replace scraper logic and adapt tests
4. Push to GitHub or Zip for delivery